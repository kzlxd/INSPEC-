{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from joblib import Parallel,delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score,log_loss,classification_report,f1_score\n",
    "import time\n",
    "import gc\n",
    "import math\n",
    "from sklearn.model_selection import KFold,StratifiedKFold,GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pickle,os\n",
    "from scipy.stats import entropy\n",
    "input_dir='../data/BAAI-INSPEC-Industrial-Quality-Prediction-Challenge_107/'\n",
    "a=os.listdir(input_dir+'INSPEC_train')\n",
    "def read_csv(idx):\n",
    "    wb = xlrd.open_workbook(input_dir+'INSPEC_train/'+a[idx])\n",
    "    sheets = wb.sheet_names()\n",
    "    df1 = pd.read_excel(input_dir+'INSPEC_train/'+a[idx], sheet_name=sheets[0])\n",
    "    df2 = pd.read_excel(input_dir+'INSPEC_train/'+a[idx], sheet_name=sheets[1])\n",
    "    df3 = pd.read_excel(input_dir+'INSPEC_train/'+a[idx], sheet_name=sheets[2])\n",
    "    df = df2.merge(df1,on=['ID_F_PROCESS'],how='left')\n",
    "    df = df3.merge(df,on =['ID_F_PHASE','ID_F_PHASE_S'],how = 'left')\n",
    "    return df\n",
    "dfs = Parallel(n_jobs=-1, verbose=10)(delayed(read_csv)(idx) for idx in range(len(a)))\n",
    "train = pd.concat(dfs, axis=0)\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "train.to_csv(input_dir+'train.csv',index=False)\n",
    "## Function to reduce the memory usage\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "train = pd.read_csv('../kernel/train.csv')\n",
    "train.rename(columns={'ID_F_PROCESS':'Product_ID'}, inplace=True)\n",
    "bad_id = train[(train['ID_F_PHASE_S'] < 3)&(train['PHASE_RESULT_STATE'] == 2)]['Product_ID'].unique()\n",
    "train1 = train[(train['ID_F_PHASE_S'] < 3)&(~train['Product_ID'].isin(bad_id))]\n",
    "train1.dropna(subset=['PROCESS_RESULT_STATE'],inplace =True)\n",
    "#train1 = train1[train1['PRODUCTGROUP_NAME'].isin(test1['PRODUCTGROUP_NAME'].unique())]\n",
    "test1 = pd.read_csv(input_dir+'final_predict_4_no_label.csv')\n",
    "df =train1.append(test1).reset_index(drop=True)\n",
    "\n",
    "#构造特征\n",
    "cat_cols, num_cols = [], []\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == object:\n",
    "        cat_cols.append(col)\n",
    "    else:\n",
    "        num_cols.append(col)\n",
    "print('Numerical columns {}, categorical columns {}'.format(len(num_cols), len(cat_cols)))\n",
    "#添加side和axis的键\n",
    "df['side_axis'] = df['SIDE'].astype('str')+df['AXIS'].astype('str')\n",
    "df['side_axis'] = df['side_axis'].map(dict(zip(df['side_axis'].unique(),range(df['side_axis'].nunique()))))\n",
    "cat_cols.remove('Product_ID')\n",
    "cat_cols.remove('RESULT_STRING')\n",
    "cat_cols.remove('SIDE')\n",
    "cat_cols.append('ID_F_PHASE_S')\n",
    "cat_cols.append('ID_F_PARAMETER_S')\n",
    "cat_cols.append('side_axis')\n",
    "#编码\n",
    "encode_col = ['AXIS','PHASE_NAME','PRODUCTGROUP_NAME','RESULT_STRING','SIDE','TYPE_NUMBER','ID_F_PHASE']\n",
    "for i in tqdm(encode_col):\n",
    "    df[i] = df[i].map(dict(zip(df[i].unique(),range(df[i].nunique()))))\n",
    "df = reduce_mem_usage(df)\n",
    "df['cha'] = df['RESULT_VALUE'] - df['LOWER_LIMIT'] / (df['UPPER_LIMIT'] - df['LOWER_LIMIT'])\n",
    "stat_cols = ['RESULT_VALUE','UPPER_LIMIT','LOWER_LIMIT']\n",
    "#df['cha'] = df['UPPER_LIMIT']-df['LOWER_LIMIT']\n",
    "\n",
    "#COUNT特征\n",
    "for f in tqdm(cat_cols):\n",
    "    print(f)\n",
    "    map_dict = dict(zip(df[f].unique(), range(df[f].nunique())))\n",
    "    df[f] = df[f].map(map_dict).fillna(-1).astype('int32')\n",
    "    df[f + '_count'] = df[f].map(df[f].value_counts())\n",
    "\n",
    "#countrank特征\n",
    "def get_most(df,col):\n",
    "    for i in col:\n",
    "        if i =='ID_F_PHASE':\n",
    "            continue\n",
    "        fit_dict = {}\n",
    "        counter =  Counter(df[i]).most_common()\n",
    "        fit_dict[i] = {k: j for (j, (k, v)) in enumerate(counter)}\n",
    "        df[i+'_num_countrank'] = df[i].map(fit_dict[i])\n",
    "    return df\n",
    "df = get_most(df,cat_cols)\n",
    "for f in tqdm(cat_cols):\n",
    "    tmp = df.groupby([f])\n",
    "    for col in (cat_cols):\n",
    "        if col == f or  f == 'ID_F_PHASE' or col =='ID_F_PHASE' or f == 'Product_ID' or col =='Product_ID':\n",
    "            continue\n",
    "            print(col)\n",
    "        df['{}_{}_nunique'.format(f, col)]= tmp[col].transform('nunique')\n",
    "        df['{}_{}_mode'.format(f, col)]= tmp[col].transform(lambda x: x.mode()[0])\n",
    "        df['{}_{}_count'.format(f, col)]= df.groupby([f, col])['Product_ID'].transform('count')\n",
    "        df['{}_{}_count_ratio'.format(col, f)] = df['{}_{}_count'.format(f, col)] / df[f + '_count'] # 比例偏好\n",
    "        df['{}_{}_count_ratio'.format(f, col)] = df['{}_{}_count'.format(f, col)] / df[col + '_count'] # 比例偏好\n",
    "        df['{}_{}_nunique_ratio_{}_count'.format(f, col, f)] = df['{}_{}_nunique'.format(f, col)] / df[f + '_count']\n",
    "        df['{}_{}_mode_count'.format(f, col)] = df['{}_{}_mode'.format(f, col)].map(df['{}_{}_mode'.format(f, col)].value_counts())\n",
    "        # mode_cols.append('{}_{}_mode'.format(f, col))\n",
    "    del tmp\n",
    "    gc.collect()\n",
    "#统计特征\n",
    "def trans(df,group_col,stat_col):\n",
    "    for i in tqdm(group_col):\n",
    "        for j in stat_col:\n",
    "            df[i+'_'+j+'_mean'] = df.groupby([i])[j].transform('mean')\n",
    "            df[i+'_'+j+'_max'] = df.groupby([i])[j].transform('max')\n",
    "            df[i+'_'+j+'_min'] = df.groupby([i])[j].transform('min')\n",
    "            df[i+'_'+j+'_medain'] = df.groupby([i])[j].transform('median')\n",
    "            df[i+'_'+j+'_std'] = df.groupby([i])[j].transform('std')\n",
    "    return df\n",
    "df = trans(df,cat_cols,stat_cols)\n",
    "#数据划分\n",
    "feature_names = [i for i in df.columns if i not in['PROCESS_RESULT_STATE','Product_ID','ID_F_PROCESS','ID_F_PHASE_num_countrank'] and 'Product_ID' not in i]\n",
    "dic={1:1,2:0}\n",
    "df['PROCESS_RESULT_STATE'] = df['PROCESS_RESULT_STATE'].map(dic)\n",
    "trn_index = ~df.PROCESS_RESULT_STATE.isnull()\n",
    "train_1 = df[trn_index][feature_names]\n",
    "y1   = df[trn_index]['PROCESS_RESULT_STATE']\n",
    "test1 = df[~trn_index][feature_names]\n",
    "print(train_1.shape,test1.shape)\n",
    "#val1\n",
    "#五折交叉验证\n",
    "param_new={\n",
    "    'objective':'binary',\n",
    " 'bagging_fraction': 0.9,\n",
    " 'feature_fraction': 0.8,\n",
    " 'learning_rate': 0.05,\n",
    " 'num_leaves': 127,\n",
    "#  'is_unbalance':'True',\n",
    "    'metric': {'auc'},\n",
    "}\n",
    "lgb_oof = np.zeros((len(train_1),))\n",
    "lgb_prediction = np.zeros((len(test1),))\n",
    "seeds = [516,3056,2028,2369,2050]\n",
    "num_model_seed = 1\n",
    "print('training')\n",
    "for model_seed in range(num_model_seed):\n",
    "    oof_lgb = np.zeros((len(train_1),))\n",
    "    prediction_lgb=np.zeros((len(test1),))\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=seeds[model_seed], shuffle=True)\n",
    " #   skf= GroupKFold(n_splits = 5)\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(skf.split(train_1, y1)):\n",
    "        print(\"fold n°{}\".format(fold_+1))\n",
    "        trn_data = lgb.Dataset(train_1.iloc[trn_idx], y1.iloc[trn_idx])\n",
    "        val_data = lgb.Dataset(train_1.iloc[val_idx], y1.iloc[val_idx])\n",
    "        num_round = 5000\n",
    "        clf1 = lgb.train(param_new, \n",
    "                        trn_data, \n",
    "                        num_round,\n",
    "                        #categorical_feature = cat_cols,\n",
    "                        valid_sets = [trn_data, val_data], \n",
    "                        verbose_eval = 200,\n",
    "                       early_stopping_rounds=200,\n",
    "                       )\n",
    "        oof_lgb[val_idx] = clf1.predict(train_1.iloc[val_idx], num_iteration=clf1.best_iteration)\n",
    "\n",
    "        prediction_lgb += clf1.predict(test1, num_iteration=clf1.best_iteration) / skf.n_splits\n",
    "        gc.collect()\n",
    "    lgb_oof += oof_lgb/ num_model_seed\n",
    "    lgb_prediction += prediction_lgb / num_model_seed\n",
    "    print('auc',roc_auc_score(y1, oof_lgb))\n",
    "print('auc',roc_auc_score(y1, lgb_oof))\n",
    "test1 = pd.read_csv(input_dir+'final_predict_4_no_label.csv')\n",
    "sub1 = pd.DataFrame()\n",
    "sub1['id'] = test1['Product_ID']\n",
    "sub1['label_'] = lgb_prediction\n",
    "sub1['label'] = sub1.groupby(['id'])['label_'].transform('mean')\n",
    "sub1_last = sub1[['id','label']].drop_duplicates('id').reset_index(drop=True)\n",
    "sub1_last['label'] = sub1_last['label'].apply(lambda x: 1 if x >= 0.965 else 0)\n",
    "print('前三步：',sub1_last['label'].value_counts())\n",
    "\n",
    "print(\"-----------------------qian shi bu--------------------------------------\")\n",
    "bad_id = train[(train['ID_F_PHASE_S'] < 10)&(train['PHASE_RESULT_STATE'] == 2)]['Product_ID'].unique()\n",
    "train1 = train[(train['ID_F_PHASE_S'] < 10)&(~train['Product_ID'].isin(bad_id))]\n",
    "train1.dropna(subset=['PROCESS_RESULT_STATE'],inplace =True)\n",
    "#train1 = train1[train1['PRODUCTGROUP_NAME'].isin(test2['PRODUCTGROUP_NAME'].unique())]\n",
    "test2 = pd.read_csv(input_dir+'final_predict_11_no_label.csv')\n",
    "df =train1.append(test2).reset_index(drop=True)\n",
    "#构造特征\n",
    "cat_cols, num_cols = [], []\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == object:\n",
    "        cat_cols.append(col)\n",
    "    else:\n",
    "        num_cols.append(col)\n",
    "print('Numerical columns {}, categorical columns {}'.format(len(num_cols), len(cat_cols)))\n",
    "#添加side和axis的键\n",
    "df['side_axis'] = df['SIDE'].astype('str')+df['AXIS'].astype('str')\n",
    "df['side_axis'] = df['side_axis'].map(dict(zip(df['side_axis'].unique(),range(df['side_axis'].nunique()))))\n",
    "#cat_cols.remove('Product_ID')\n",
    "cat_cols.remove('RESULT_STRING')\n",
    "cat_cols.remove('SIDE')\n",
    "cat_cols.append('ID_F_PHASE_S')\n",
    "cat_cols.append('ID_F_PARAMETER_S')\n",
    "cat_cols.append('side_axis')\n",
    "#编码\n",
    "encode_col = ['AXIS','PHASE_NAME','PRODUCTGROUP_NAME','RESULT_STRING','SIDE','TYPE_NUMBER','ID_F_PHASE']\n",
    "for i in tqdm(encode_col):\n",
    "    df[i] = df[i].map(dict(zip(df[i].unique(),range(df[i].nunique()))))\n",
    "df = reduce_mem_usage(df)\n",
    "stat_cols = ['RESULT_VALUE','UPPER_LIMIT','LOWER_LIMIT']\n",
    "df['cha'] = df['UPPER_LIMIT']-df['LOWER_LIMIT']\n",
    "#COUNT特征\n",
    "for f in tqdm(cat_cols):\n",
    "    print(f)\n",
    "    map_dict = dict(zip(df[f].unique(), range(df[f].nunique())))\n",
    "    df[f] = df[f].map(map_dict).fillna(-1).astype('int32')\n",
    "    df[f + '_count'] = df[f].map(df[f].value_counts())\n",
    "\n",
    "#countrank特征\n",
    "def get_most(df,col):\n",
    "    for i in col:\n",
    "        if i =='ID_F_PHASE':\n",
    "            continue\n",
    "        fit_dict = {}\n",
    "        counter =  Counter(df[i]).most_common()\n",
    "        fit_dict[i] = {k: j for (j, (k, v)) in enumerate(counter)}\n",
    "        df[i+'_num_countrank'] = df[i].map(fit_dict[i])\n",
    "    return df\n",
    "df = get_most(df,cat_cols)\n",
    "# #类别特征\n",
    "for f in tqdm(cat_cols):\n",
    "    tmp = df.groupby([f])\n",
    "    for col in (cat_cols):\n",
    "        if col == f or  f == 'ID_F_PHASE' or col =='ID_F_PHASE' or f == 'Product_ID' or col =='Product_ID':\n",
    "            continue\n",
    "            print(col)\n",
    "        df['{}_{}_nunique'.format(f, col)]= tmp[col].transform('nunique')\n",
    "        df['{}_{}_mode'.format(f, col)]= tmp[col].transform(lambda x: x.mode()[0])\n",
    "        df['{}_{}_count'.format(f, col)]= df.groupby([f, col])['Product_ID'].transform('count')\n",
    "        df['{}_{}_count_ratio'.format(col, f)] = df['{}_{}_count'.format(f, col)] / df[f + '_count'] # 比例偏好\n",
    "        df['{}_{}_count_ratio'.format(f, col)] = df['{}_{}_count'.format(f, col)] / df[col + '_count'] # 比例偏好\n",
    "        df['{}_{}_nunique_ratio_{}_count'.format(f, col, f)] = df['{}_{}_nunique'.format(f, col)] / df[f + '_count']\n",
    "        df['{}_{}_mode_count'.format(f, col)] = df['{}_{}_mode'.format(f, col)].map(df['{}_{}_mode'.format(f, col)].value_counts())\n",
    "        # mode_cols.append('{}_{}_mode'.format(f, col))\n",
    "    del tmp\n",
    "    gc.collect()\n",
    "#统计特征\n",
    "def trans(df,group_col,stat_col):\n",
    "    for i in tqdm(group_col):\n",
    "        for j in stat_col:\n",
    "            df[i+'_'+j+'_mean'] = df.groupby([i])[j].transform('mean')\n",
    "            df[i+'_'+j+'_max'] = df.groupby([i])[j].transform('max')\n",
    "            df[i+'_'+j+'_min'] = df.groupby([i])[j].transform('min')\n",
    "            df[i+'_'+j+'_medain'] = df.groupby([i])[j].transform('median')\n",
    "            df[i+'_'+j+'_std'] = df.groupby([i])[j].transform('std')\n",
    "    return df\n",
    "df = trans(df,cat_cols,stat_cols)\n",
    "#数据划分\n",
    "feature_names = [i for i in df.columns if i not in['PROCESS_RESULT_STATE','Product_ID','ID_F_PROCESS','ID_F_PHASE_num_countrank']and 'Product_ID' not in i]\n",
    "dic={1:1,2:0}\n",
    "df['PROCESS_RESULT_STATE'] = df['PROCESS_RESULT_STATE'].map(dic)\n",
    "trn_index = ~df.PROCESS_RESULT_STATE.isnull()\n",
    "train_1 = df[trn_index][feature_names]\n",
    "y1   = df[trn_index]['PROCESS_RESULT_STATE']\n",
    "test2 = df[~trn_index][feature_names]\n",
    "print(train_1.shape,test2.shape)\n",
    "#val2\n",
    "#五折交叉验证\n",
    "param_new={\n",
    "    'objective':'binary',\n",
    " 'bagging_fraction': 0.7808187852160583,\n",
    " 'feature_fraction': 0.3043366780447281,\n",
    " 'lambda_l1': 0.25,\n",
    " 'lambda_l2': 0.4,\n",
    " 'learning_rate': 0.06259858514656305,\n",
    " 'max_bins': 59,\n",
    " 'min_data_in_leaf': 21,\n",
    " 'num_leaves': 102,\n",
    "#  'is_unbalance':'True',\n",
    "    'metric': {'auc'},\n",
    "}\n",
    "lgb_oof = np.zeros((len(train_1),))\n",
    "lgb_prediction1 = np.zeros((len(test2),))\n",
    "seeds = [516,3056,2028,2369,2050]\n",
    "num_model_seed = 1\n",
    "print('training')\n",
    "for model_seed in range(num_model_seed):\n",
    "    oof_lgb = np.zeros((len(train_1),))\n",
    "    prediction_lgb1=np.zeros((len(test2),))\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=seeds[model_seed], shuffle=True)\n",
    " #   skf= GroupKFold(n_splits = 5)\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(skf.split(train_1, y1)):\n",
    "        print(\"fold n°{}\".format(fold_+1))\n",
    "        trn_data = lgb.Dataset(train_1.iloc[trn_idx], y1.iloc[trn_idx])\n",
    "        val_data = lgb.Dataset(train_1.iloc[val_idx], y1.iloc[val_idx])\n",
    "        num_round = 5000\n",
    "        clf1 = lgb.train(param_new, \n",
    "                        trn_data, \n",
    "                        num_round,\n",
    "                        #categorical_feature = cat_cols,\n",
    "                        valid_sets = [trn_data, val_data], \n",
    "                        verbose_eval = 200,\n",
    "                       early_stopping_rounds=200,\n",
    "                       )\n",
    "        oof_lgb[val_idx] = clf1.predict(train_1.iloc[val_idx], num_iteration=clf1.best_iteration)\n",
    "\n",
    "        prediction_lgb1 += clf1.predict(test2, num_iteration=clf1.best_iteration) / skf.n_splits\n",
    "        gc.collect()\n",
    "    lgb_oof += oof_lgb/ num_model_seed\n",
    "    lgb_prediction1 += prediction_lgb1 / num_model_seed\n",
    "    print('auc',roc_auc_score(y1, oof_lgb))\n",
    "print('auc',roc_auc_score(y1, lgb_oof))\n",
    "test2 = pd.read_csv(input_dir+'final_predict_11_no_label.csv')\n",
    "sub2 = pd.DataFrame()\n",
    "sub2['id'] = test2['Product_ID']\n",
    "\n",
    "sub2['label_'] = lgb_prediction1\n",
    "sub2['label'] = sub2.groupby(['id'])['label_'].transform('mean')\n",
    "sub2_last = sub2[['id','label']].drop_duplicates('id').reset_index(drop=True)\n",
    "sub2_last['label'] = sub2_last['label'].apply(lambda x: 1 if x >=0.97 else 0)\n",
    "print('前十步：',sub2_last['label'].value_counts())\n",
    "sub = sub1_last.append(sub2_last).reset_index(drop=True)\n",
    "d = {0:2,1:1}\n",
    "sub['label'] = sub['label'].map(d)\n",
    "sub.to_csv('../work/submission.csv',index=False)\n",
    "print('result',sub.label.value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
